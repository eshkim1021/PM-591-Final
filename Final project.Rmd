---
title: "Final project"
date: "Due April 30, 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tidyverse)
library(data.table)
library(mlr)
library(dplyr)
library(randomForest)
library(rpart)

data <- read.csv("NIS2012-200k.csv", header = TRUE, stringsAsFactors = TRUE)

data.dt <- data.frame(data)

```

## __Introduction__

  The National Impatient Sample (NIS) data, collected by the Healthcare Cost and Utilization Project (HCUP), is the largest publicly available dataset that contains information on inpatient healthcare in hospitals throughout the United States. The NIS is used by policymakers and health officials to make national estimates of healthcare utilization, and observe key features of inpatient care. The NIS was first started in 1998 by the Healthcare Cost and Utilization Project, and contains information such as patient demographics, classification of diseases, total hospital bill, length of stay, and many other features that characterize hospital care. The goal of this assignment will be to build a model to predict impatient mortality an determine what factors contribute a increased risk of death during hospitalization. 

  The data that will be used in this assignment consists of a random subset of 200,000 patients from the 2012 National Impatient Sample. The data was taken from the Healthcare Cost and Ultilization Project (HCUP), which is the largest collection of hospital care data in the United States. The data was taken from discharge records from all hospitals that are participating with the HCUP, and use state guidelines to help identify the hospitals that qualify for the data collection process. 47 states and the District of Columbia participate in the NIS, and data is available for hospitals in those states. The outcome of interest is the inpatient mortality, of whether the patient died during the period of hospitalization. Features such as patient demographic, severity of disease, risk of mortality, and comorbidities were incorpated to determine if a patient was likely to die during hospitalization. This can be used to identify features that increase the risk of patient mortality in hospitals and seek to prevent such deaths in the future. 


## __Methods__

First the relevant features to the outcome of interest was sorted out from the 175 original features that were present. 

```{r, include = FALSE}
names <- c("DIED", "APRDRG_Risk_Mortality","AGE","APRDRG_Severity","CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","LOS","NCHRONIC","NDX","NEOMAT","PAY1","RACE","YEAR","ZIPINC_QRTL","ORPROC") #list the features that will be included 


refine_data <- data.dt %>% select(names)

```

Then then data was then reevaluated and factors were added when necessary. 

```{r, include = FALSE}
refine_data$DIED <- factor(refine_data$DIED,
                           levels = c(0,1),
                           labels = c("Alive","Died"))

refine_data$APRDRG_Risk_Mortality <- factor(refine_data$APRDRG_Risk_Mortality,
                                            levels = c(0,1,2,3,4), 
                                            labels = c("Not specified","Minor Likelihood","Moderate Likelihood","Major Likelihood","Extreme Likelihood"))


refine_data$APRDRG_Severity <- factor(refine_data$APRDRG_Severity,
                                      levels = c(0,1,2,3,4),
                                      labels = c("Not specified","Minor Loss of Function","Moderate Loss of Function","Major Loss of Function","Extreme Loss of Function"))

factor_names <- c("CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","NEOMAT")

refine_data[factor_names] <- lapply(refine_data[factor_names],factor)

refine_data$PAY1 <- factor(refine_data$PAY1,
                           levels = c(1,2,3,4,5,6),
                           labels = c("Medicare","Medicaid","Private","Self-Pay","No Charge","Other"))

refine_data$RACE <- factor(refine_data$RACE,
                           levels = c(1,2,3,4,5,6),
                           labels = c("White","Black","Hispanic","Asian","Native American","Other"))


refine_data <- na.omit(refine_data)

refine_data$AGE <- as.integer(refine_data$AGE)

refine_data$LOS <- as.integer(refine_data$LOS)

summary(refine_data)

```


Out of the 175 possible features that were present in the original dataset, only 44 variables were selected to be included in analysis and model building. These 44 include data regarding patient demographics (age, race, gender), comorbidities (such as alcohol abuse and COPD), and the risks of patient mortality. Each variable was examined and was made into factor variables as was appropriate. A majority of the features were converted into dummy variables, however some remained as strings and integers. In examining the missing data, there was less than 1% of the total sample size that was missing from the target variable, whether the patient died. Because the sample was small compared to the dataset, the missing values of the target variable were removed before the analysis.


```{r, include = FALSE}
#make task for log reg 
data_tsk <- makeClassifTask(id = "Paitent Mortality", data = refine_data, target = "DIED")
```



```{r, include = FALSE}
#make log learner 
data_learn_log <- makeLearner("classif.logreg",
                              fix.factors.prediction = TRUE,
                              predict.type = "prob")

holdout_desc <- makeResampleDesc(method = "Holdout", stratify = TRUE)

set.seed(301)
log_split <- makeResampleInstance(holdout_desc,data_tsk, split = 0.7)

log_train <- log_split$train.inds[[1]];log_test <- log_split$test.inds[[1]]


#use forward subset to determine best result 
ctrl_forward <- makeFeatSelControlSequential(method = "sfs", alpha = 0.01)

log_forward_cv <- makeResampleDesc("CV",iters = 5L)

log_forward <- selectFeatures(learner = data_learn_log,
                              task = data_tsk,
                              resampling     = log_forward_cv,
                              measures = auc, 
                              control = ctrl_forward,
                              show.info = TRUE)

#only Risk of Mortality found to be inmportant, but will include other demographic factors 
analyzeFeatSelResult(log_forward)

#Risk of Mortality, Race, and Length of Stay 
forward_log_data <- refine_data %>% select("DIED","APRDRG_Risk_Mortality","LOS","AGE","RACE")

#create new task for the new dataset 
log_for_tsk <- makeClassifTask(id = "Paitent Mortality", data = forward_log_data,
                               target = "DIED")

log_for_train <- train(data_learn_log,log_for_tsk, subset = log_train)

log_for_predict <- predict(log_for_train, task = log_for_tsk, subset = log_test)

calculateROCMeasures(log_for_predict)

performance(log_for_predict, measures = list(mmce,acc))

for_log_crossval <- crossval(data_learn_log,log_for_tsk,iters = 10L, stratify = TRUE, measures = mmce)

for_log_crossval$aggr
```

### __Logistic Regression__

I will be comparing 3 different methods to build a predictive model for patient mortality. The first will be logistic regression model. The logistic regression model is one of the most commonly used and basic binary classifiers. Because the desired goal is to determine if a patient died during their hospitalization, the outcome is a binary outcome.  Given the extremely large sample size of the data with around 200,000 observations, both the training and testing sets will be large enough to ensure an accurate prediction model. 

Forward selection was used to determine the features that will be included in the logistic regression model. According to the forward selection process, only the __APRDRG_Risk_Mortality__, a factor variable that characterizes the risk of patient mortality, was determined to be significant in the data. However, the race variable was also included to determine the effect of patient demographics on mortality. There will only be a couple of features included in the actual logistic prediction model, therefore the model will be a simpler one indicating that the model will have a higher bias. However, the large sample size of the data and the use of cross validation will be used to determine the accuracy of the results. I will be using the misclassification error and the AUC as performance metrics to determine the effectiveness of the logistic model and compare it to other models that I wiil be using. 

```{r,include = FALSE}
library('pROC')
data_glm <- glm(DIED~APRDRG_Risk_Mortality + RACE + LOS, family = 'binomial',data = forward_log_data[log_train,])

pred_glm <- factor(predict(data_glm,newdata = forward_log_data[log_test, ],type = 'response') >0.5)


predict_prob_train <- predict(data_glm, newdata = forward_log_data[log_train, ])
predict_prob_test <- predict(data_glm,newdata = forward_log_data[log_test, ])

roc_glm_train <- roc(forward_log_data[log_train,]$DIED,predict_prob_train, ci = TRUE, of = 'auc')
roc_glm_test <- roc(forward_log_data[log_test, ]$DIED,predict_prob_test, ci = TRUE, of = 'auc')
```

K-fold cross validation will be used to reduce the error that comes from different training/testing splits. The resulting misclassification error from the k-fold cross validation will be compared with the misclassification error from the initial training/testing split. 

### __Balanced Random Forests__

The data itself is very unbalanced, with 184,598 patients that were successfully discharged compared to the 3,412 that died in the hospital. This could lead to an optimistically low misclassification error. Therefore, balanced random forests will be used to help balance the two binary outcomes and correct over optomistic misclassification errors. 

I will be using all of the 44 variables that were selected form the original dataset in the balanced random forest model. All of the variables are included because the random forest model will tune the parameters and adjust the model according to which variables are considered important. The variable importance plot generated from the balanced random forest model will be compared to the variables that were considered important in the forward selection algorithm used in the logistic regression. The AUC will be used as a performance metric as the goal of the model is to correctly predict patient mortality. 


```{r,include = FALSE}
library(randomForest)


data_rf <- randomForest(DIED~.,data = refine_data[log_train, ],
                        mtry = sqrt(44),
                        ntree = 500,
                        strata = refine_data$DIED[log_train],
                        sampsize = c(2274,2274))

data_rf

rf_roc_train <- roc(refine_data[log_train, ]$DIED, data_rf$votes[,1])

auc(rf_roc_train)

rf_predict_test <- predict(data_rf,
                           newdata = refine_data[log_test, ],
                           type = 'prob')

rf_roc_test <- roc(refine_data[log_test, ]$DIED,rf_predict_test[,1])

auc(rf_roc_test)
ci(rf_roc_test)

```

### __Lasso Regression__

Lasso regression will also be used to build a predictive model for determining patient mortality. Lasso regression was chosen over ridge regression because it is likely that only a small number of predictors will be significant in determining patient mortality than the all of the features that we have available. The regression model itself will chose which variables are important and act similarly to a feature selection algorithm. Therefore, we will continue to increase the tuning parameter to determine which parameters are important in determining patient mortality. 

Cross validation will be used to tune the parameters in the LASSO regression and the misclassification error and AUC will be calculated to compare the performance o the LASSO regression model with the logistic regression and balanced random forest model. 


```{r,include = FALSE}
library(glmnet)

glmnet.data <- refine_data
glmnet.data$DIED <- as.numeric(refine_data$DIED)

y <- refine_data$DIED
x <- model.matrix(refine_data$DIED~., data = refine_data)[, -1]

dat_CVlasso <- cv.glmnet(x,y,family = "binomial",alpha = 1, type.measure = "auc")

dat_CVlasso_coef <- coef(dat_CVlasso)
round(dat_CVlasso_coef,2)

auc_lasso <- max(dat_CVlasso$cvm)
```



```{r, include = FALSE}

learn_CV_lasso <- makeLearner("classif.cvglmnet",
                              fix.factors.prediction = TRUE,
                              predict.type ="prob",
                              alpha = 1,
                              type.measure = 'auc')

data_CV_lasso_train <- train(learn_CV_lasso, task = data_tsk,subset = log_train) 

auc_train <- max(data_CV_lasso_train$learner.model$cvm)
 
lambda_min <- data_CV_lasso_train$learner.model$lambda.min

data_CV_lasso_min_lnr<- makeLearner("classif.glmnet",
                                    fix.factors.prediction = TRUE,
                                    predict.type = "prob",
                                    alpha = 1,
                                    lambda = lambda_min)

data_CV_lasso_predict <- predict(data_CV_lasso_train, task = data_tsk)

performance(data_CV_lasso_predict, measures = mmce)
```



## __Results__

### __Logistic Regression__:


Below is the AUC and associated 95% Confidence Interval for the logistic regression on the training data. 

```{r, echo = FALSE}
auc(roc_glm_train)
ci(roc_glm_train)
```

Below is the AUC and associated 95% confidence interval for the logistic regression on the testing data. 

```{r, echo = FALSE}
auc(roc_glm_test)
ci(roc_glm_test)
```

The plot below shows the ROC curve for both the training and testing data for the logistic regression model on patient mortality. 

```{r, echo = FALSE}
plot(roc_glm_train, lty = 1, lwd = 2, col = 'red4',cex.axis = 1.3, cex.lab = 1.3,main = "ROC Curve for Patient Mortality (Logistic Regression)")
lines(roc_glm_test,lty = 1, lwd = 2, col = 'blue4', cex.axis = 1.3, cex.lab = 1.3)
legend("bottomright", legend = c("Train","Test"), col = c('red4','blue4'),lty = 1)
```


The table below shows the parameter estimates and the associated p-values for the logistic regression. The risk of mortality variable is statistically significant across almost all of its factor levels, and the length of stay variable is also statistically significant. The race variable is statistically significant only if the individual is Black. However, the parameter estimates for Race and Length of Stay are not very large compared to that of the Risk Mortality, indicating that the Risk Mortality has the highest influence in determining the probability of patient mortality.

```{r, echo = FALSE}
coef(summary(data_glm))[ ,c(1,4)]
```


### __Balanced Random Forests__: 

```{r, echo = FALSE}
auc(rf_roc_train)
ci(rf_roc_train)
```

```{r, ehco = FALSE}
auc(rf_roc_test)
ci(rf_roc_test)

```

The plot below shows the ROC curve for both the training and testing data for the balanced random forest method in predicting patient mortality. 

```{r, echo = FALSE}
plot(rf_roc_train,lwd = 2, col = 'red4',cex.axis = 1.3, cex.lab = 1.3, main = "ROC Curve for Patient Mortality (Balanced Random Forest)")
lines(rf_roc_test, lwd = 2, col = 'blue4', cex.axis = 1.3, cex.lab = 1.3)
legend("bottomright", legend = c("Train","Test"), col = c('red4','blue4'),lty = 1)
```

Below is the variable importance in predicting Patient Mortality using the balanced random forests method. This indicates that the Risk Mortality and the Severity of the disease are the most important variables in predicting patient mortality. Other significant variables include age and the number of diagnoses coded on the patient's health record. 

```{r, echo = FALSE}
varImpPlot(data_rf, cex = 0.7, pt.cex = 1.2, n.var = 20, pch = 16, col = 'red4', main = "Variable Importance for Patient Mortality")
```

### __Lasso Regression__

Below is the cross-validated AUC for the LASSO regression model in predicting patient mortality. 

```{r, echo = FALSE}
auc_lasso
```

Below is the cross-validated misclassification error for the LASSO regression model in predicting patient mortality. 

```{r, echo = FALSE}
performance(data_CV_lasso_predict, measures = mmce)
```

Below is the plot of the cross-validated tuning process for the regression model. 

```{r, echo = FALSE}
plot(dat_CVlasso,cex.lab = 1, cex.axis = 1)
```



### Conclusions/discussion
Discuss whether and why the prediction model(s) developed achieved sufficient high accuracy to be usefully deployed to predict new observations.

#Additional notes for those using the NIS data
The data provided consists of a random subset of 200,000 patients from 2012 from the National Impatient Sample (NIS) data collected by the Healthcare Cost and Utilization Project (HCUP). You can find information on the HCUP database at <https://www.hcup-us.ahrq.gov>. You can choose to develop a model to predict death during hospitalization also known as impatient mortality (variable DIED in the dataset) or hospital length of stay (variable LOS in the dataset). For extra credit, you can also choose to predict both. The dataset has a relatively large number of variables. In the provided data dictionary I preselected variables (highlighted) which are both availabe (not all variables in the dictionary are available for 2012) which might be relevant for predicting impatient mortality and/or hospital length of stay. Based on their description and additional info from the HCUP site you should choose which variables among the preselected ones you will consider as features/predictors. You don't have to use them all. There maybe variables that are redundant (capture pretty much the same info others already capture), variables that are too complex (e.g. categorical with way too many levels), or that based on your judgment are unlikely to be important. Be aware that the data is real and has not been pre-processed in any way and you will have to do some data cleaning. For example, you should carefully check the variables you consider as possible predictors for correctness of type (e.g. many numeric variables will be read in as factor variables when you use ``read.csv``), outliers, missing observations, nonsensical values, etc.



