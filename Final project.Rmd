---
title: "PM 591 Final project"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tidyverse)
library(data.table)
library(mlr)
library(dplyr)
library(randomForest)
library(rpart)

data <- read.csv("NIS2012-200k.csv", header = TRUE, stringsAsFactors = TRUE)

data.dt <- data.frame(data)

```

## __Introduction__

  The National Impatient Sample (NIS) data, collected by the Healthcare Cost and Utilization Project (HCUP), is the largest publicly available dataset that contains information on inpatient healthcare in hospitals throughout the United States. The NIS is used by policymakers and health officials to make national estimates of healthcare utilization, and observe key features of inpatient care. The NIS was first started in 1998 by the Healthcare Cost and Utilization Project, and contains information such as patient demographics, classification of diseases, total hospital bill, length of stay, and many other features that characterize hospital care. The goal of this assignment will be to build a model to predict impatient mortality an determine what factors contribute a increased risk of death during hospitalization. 

  The data that will be used in this assignment consists of a random subset of 200,000 patients from the 2012 National Impatient Sample. The data was taken from the Healthcare Cost and Ultilization Project (HCUP), which is the largest collection of hospital care data in the United States. The data was taken from discharge records from all hospitals that are participating with the HCUP, and use state guidelines to help identify the hospitals that qualify for the data collection process. 47 states and the District of Columbia participate in the NIS, and data is available for hospitals in those states. The outcome of interest is the inpatient mortality, of whether the patient died during the period of hospitalization. Features such as patient demographic, severity of disease, risk of mortality, and comorbidities were incorpated to determine if a patient was likely to die during hospitalization. This can be used to identify features that increase the risk of patient mortality in hospitals and seek to prevent such deaths in the future. 


## __Methods__

First the relevant features to the outcome of interest was sorted out from the 175 original features that were present. 

```{r, include = FALSE}
names <- c("DIED", "APRDRG_Risk_Mortality","AGE","APRDRG_Severity","CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","LOS","NCHRONIC","NDX","NEOMAT","PAY1","RACE","YEAR","ZIPINC_QRTL","ORPROC") #list the features that will be included 


refine_data <- data.dt %>% select(names)

```

Then then data was then reevaluated and factors were added when necessary. 

```{r, include = FALSE}
refine_data$DIED <- factor(refine_data$DIED,
                           levels = c(0,1),
                           labels = c("Alive","Died"))

refine_data$APRDRG_Risk_Mortality <- factor(refine_data$APRDRG_Risk_Mortality,
                                            levels = c(0,1,2,3,4), 
                                            labels = c("Not specified","Minor Likelihood","Moderate Likelihood","Major Likelihood","Extreme Likelihood"))


refine_data$APRDRG_Severity <- factor(refine_data$APRDRG_Severity,
                                      levels = c(0,1,2,3,4),
                                      labels = c("Not specified","Minor Loss of Function","Moderate Loss of Function","Major Loss of Function","Extreme Loss of Function"))

factor_names <- c("CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","NEOMAT")

refine_data[factor_names] <- lapply(refine_data[factor_names],factor)

refine_data$PAY1 <- factor(refine_data$PAY1,
                           levels = c(1,2,3,4,5,6),
                           labels = c("Medicare","Medicaid","Private","Self-Pay","No Charge","Other"))

refine_data$RACE <- factor(refine_data$RACE,
                           levels = c(1,2,3,4,5,6),
                           labels = c("White","Black","Hispanic","Asian","Native American","Other"))


refine_data <- na.omit(refine_data)

refine_data$AGE <- as.integer(refine_data$AGE)

refine_data$LOS <- as.integer(refine_data$LOS)

summary(refine_data)

```


Out of the 175 possible features that were present in the original dataset, only 44 variables were selected to be included in analysis and model building. These 44 include data regarding patient demographics (age, race, gender), comorbidities (such as alcohol abuse and COPD), and the risks of patient mortality. Each variable was examined and was made into factor variables as was appropriate. A majority of the features were converted into dummy variables, however some remained as strings and integers. In examining the missing data, there was less than 1% of the total sample size that was missing from the target variable, whether the patient died. Because the sample was small compared to the dataset, the missing values of the target variable were removed before the analysis.


```{r, include = FALSE}
#make task for log reg 
data_tsk <- makeClassifTask(id = "Paitent Mortality", data = refine_data, target = "DIED")
```



```{r, include = FALSE}
#make log learner 
data_learn_log <- makeLearner("classif.logreg",
                              fix.factors.prediction = TRUE,
                              predict.type = "prob")

holdout_desc <- makeResampleDesc(method = "Holdout", stratify = TRUE)

set.seed(301)
log_split <- makeResampleInstance(holdout_desc,data_tsk, split = 0.7)

log_train <- log_split$train.inds[[1]];log_test <- log_split$test.inds[[1]]


#use forward subset to determine best result 
ctrl_forward <- makeFeatSelControlSequential(method = "sfs", alpha = 0.01)

log_forward_cv <- makeResampleDesc("CV",iters = 5L)

log_forward <- selectFeatures(learner = data_learn_log,
                              task = data_tsk,
                              resampling     = log_forward_cv,
                              measures = auc, 
                              control = ctrl_forward,
                              show.info = TRUE)

#only Risk of Mortality found to be inmportant, but will include other demographic factors 
analyzeFeatSelResult(log_forward)

#Risk of Mortality, Race, and Length of Stay 
forward_log_data <- refine_data %>% select("DIED","APRDRG_Risk_Mortality","LOS","AGE","RACE")

#create new task for the new dataset 
log_for_tsk <- makeClassifTask(id = "Paitent Mortality", data = forward_log_data,
                               target = "DIED")

log_for_train <- train(data_learn_log,log_for_tsk, subset = log_train)

log_for_predict <- predict(log_for_train, task = log_for_tsk, subset = log_test)

calculateROCMeasures(log_for_predict)

log_for_perform <- performance(log_for_predict, measures = list(mmce,acc))

for_log_crossval <- crossval(data_learn_log,log_for_tsk,iters = 10L, stratify = TRUE, measures = mmce)

for_log_crossval$aggr
```

### __Logistic Regression__

I will be comparing 3 different methods to build a predictive model for patient mortality. The first will be logistic regression model. The logistic regression model is one of the most commonly used and basic binary classifiers. Because the desired goal is to determine if a patient died during their hospitalization, the outcome is a binary outcome.  Given the extremely large sample size of the data with around 200,000 observations, both the training and testing sets will be large enough to ensure an accurate prediction model. 

Forward selection was used to determine the features that will be included in the logistic regression model. According to the forward selection process, only the __APRDRG_Risk_Mortality__, a factor variable that characterizes the risk of patient mortality, was determined to be significant in the data. However, the race variable was also included to determine the effect of patient demographics on mortality. There will only be a couple of features included in the actual logistic prediction model, therefore the model will be a simpler one indicating that the model will have a higher bias. However, the large sample size of the data and the use of cross validation will be used to determine the accuracy of the results. I will be using the misclassification error and the AUC as performance metrics to determine the effectiveness of the logistic model and compare it to other models that I wiil be using. 

```{r,include = FALSE}
library('pROC')
data_glm <- glm(DIED~APRDRG_Risk_Mortality + RACE + LOS, family = 'binomial',data = forward_log_data[log_train,])

pred_glm <- factor(predict(data_glm,newdata = forward_log_data[log_test, ],type = 'response') >0.5)


predict_prob_train <- predict(data_glm, newdata = forward_log_data[log_train, ])
predict_prob_test <- predict(data_glm,newdata = forward_log_data[log_test, ])

roc_glm_train <- roc(forward_log_data[log_train,]$DIED,predict_prob_train, ci = TRUE, of = 'auc')
roc_glm_test <- roc(forward_log_data[log_test, ]$DIED,predict_prob_test, ci = TRUE, of = 'auc')
```

K-fold cross validation will be used to reduce the error that comes from different training/testing splits. The resulting misclassification error from the k-fold cross validation will be compared with the misclassification error from the initial training/testing split. 

### __Balanced Random Forests__

The data itself is very unbalanced, with 184,598 patients that were successfully discharged compared to the 3,412 that died in the hospital. This could lead to an optimistically low misclassification error. Therefore, balanced random forests will be used to help balance the two binary outcomes and correct over optomistic misclassification errors. 

I will be using all of the 44 variables that were selected form the original dataset in the balanced random forest model. All of the variables are included because the random forest model will tune the parameters and adjust the model according to which variables are considered important. The variable importance plot generated from the balanced random forest model will be compared to the variables that were considered important in the forward selection algorithm used in the logistic regression. The AUC will be used as a performance metric as the goal of the model is to correctly predict patient mortality. 


```{r,include = FALSE}
library(randomForest)


data_rf <- randomForest(DIED~.,data = refine_data[log_train, ],
                        mtry = sqrt(44),
                        ntree = 500,
                        strata = refine_data$DIED[log_train],
                        sampsize = c(2274,2274))

data_rf

rf_roc_train <- roc(refine_data[log_train, ]$DIED, data_rf$votes[,1])

auc(rf_roc_train)

rf_predict_test <- predict(data_rf,
                           newdata = refine_data[log_test, ],
                           type = 'prob')

rf_roc_test <- roc(refine_data[log_test, ]$DIED,rf_predict_test[,1])

auc(rf_roc_test)
ci(rf_roc_test)

```

### __Lasso Regression__

Lasso regression will also be used to build a predictive model for determining patient mortality. Lasso regression was chosen over ridge regression because it is likely that only a small number of predictors will be significant in determining patient mortality than the all of the features that we have available. The regression model itself will chose which variables are important and act similarly to a feature selection algorithm. Therefore, we will continue to increase the tuning parameter to determine which parameters are important in determining patient mortality. 

Cross validation will be used to tune the parameters in the LASSO regression and the misclassification error and AUC will be calculated to compare the performance o the LASSO regression model with the logistic regression and balanced random forest model. 


```{r,include = FALSE}
library(glmnet)

glmnet.data <- refine_data
glmnet.data$DIED <- as.numeric(refine_data$DIED)

y <- refine_data$DIED
x <- model.matrix(refine_data$DIED~., data = refine_data)[, -1]

dat_CVlasso <- cv.glmnet(x,y,family = "binomial",alpha = 1, type.measure = "auc")

dat_CVlasso_coef <- coef(dat_CVlasso)
round(dat_CVlasso_coef,2)

auc_lasso <- max(dat_CVlasso$cvm)
```



```{r, include = FALSE}

learn_CV_lasso <- makeLearner("classif.cvglmnet",
                              fix.factors.prediction = TRUE,
                              predict.type ="prob",
                              alpha = 1,
                              type.measure = 'auc')

data_CV_lasso_train <- train(learn_CV_lasso, task = data_tsk,subset = log_train) 

auc_train <- max(data_CV_lasso_train$learner.model$cvm)
 
lambda_min <- data_CV_lasso_train$learner.model$lambda.min

data_CV_lasso_min_lnr<- makeLearner("classif.glmnet",
                                    fix.factors.prediction = TRUE,
                                    predict.type = "prob",
                                    alpha = 1,
                                    lambda = lambda_min)

data_CV_lasso_predict <- predict(data_CV_lasso_train, task = data_tsk)

performance(data_CV_lasso_predict, measures = mmce)
```



## __Results__

### __Logistic Regression__:


Below is the AUC and associated 95% Confidence Interval for the logistic regression on the training data. 

```{r, echo = FALSE}
auc(roc_glm_train)
ci(roc_glm_train)
```

Below is the AUC and associated 95% confidence interval for the logistic regression on the testing data. 

```{r, echo = FALSE}
auc(roc_glm_test)
ci(roc_glm_test)
```

The plot below shows the ROC curve for both the training and testing data for the logistic regression model on patient mortality. 

```{r, echo = FALSE}
plot(roc_glm_train, lty = 1, lwd = 2, col = 'red4',cex.axis = 1.3, cex.lab = 1.3,main = "ROC Curve for Patient Mortality (Logistic Regression)")
lines(roc_glm_test,lty = 1, lwd = 2, col = 'blue4', cex.axis = 1.3, cex.lab = 1.3)
legend("bottomright", legend = c("Train","Test"), col = c('red4','blue4'),lty = 1)
```


The table below shows the parameter estimates and the associated p-values for the logistic regression. The risk of mortality variable is statistically significant across almost all of its factor levels, and the length of stay variable is also statistically significant. The race variable is statistically significant only if the individual is Black. However, the parameter estimates for Race and Length of Stay are not very large compared to that of the Risk Mortality, indicating that the Risk Mortality has the highest influence in determining the probability of patient mortality.

```{r, echo = FALSE}
coef(summary(data_glm))[ ,c(1,4)]
```


### __Balanced Random Forests__: 

```{r, echo = FALSE}
auc(rf_roc_train)
ci(rf_roc_train)
```

```{r, ehco = FALSE}
auc(rf_roc_test)
ci(rf_roc_test)

```

The plot below shows the ROC curve for both the training and testing data for the balanced random forest method in predicting patient mortality. 

```{r, echo = FALSE}
plot(rf_roc_train,lwd = 2, col = 'red4',cex.axis = 1.3, cex.lab = 1.3, main = "ROC Curve for Patient Mortality (Balanced Random Forest)")
lines(rf_roc_test, lwd = 2, col = 'blue4', cex.axis = 1.3, cex.lab = 1.3)
legend("bottomright", legend = c("Train","Test"), col = c('red4','blue4'),lty = 1)
```

Below is the variable importance in predicting Patient Mortality using the balanced random forests method. This indicates that the Risk Mortality and the Severity of the disease are the most important variables in predicting patient mortality. Other significant variables include age and the number of diagnoses coded on the patient's health record. 

```{r, echo = FALSE}
varImpPlot(data_rf, cex = 0.7, pt.cex = 1.2, n.var = 20, pch = 16, col = 'red4', main = "Variable Importance for Patient Mortality")
```

### __Lasso Regression__

Below is the cross-validated AUC for the LASSO regression model in predicting patient mortality. 

```{r, echo = FALSE}
auc_lasso
```

Below is the cross-validated misclassification error for the LASSO regression model in predicting patient mortality. 

```{r, echo = FALSE}
performance(data_CV_lasso_predict, measures = mmce)
```

Below is the plot of the cross-validated tuning process for the regression model. The numnber of coefficents in the top of the graph is greater than the 44 variables in the data because of factor variables that have multiple levels. 

```{r, echo = FALSE}
plot(dat_CVlasso,cex.lab = 1, cex.axis = 1)
```



### Conclusions/discussion

Across all three prediction models, the AUC was above 0.90 indicated that the prediction models were more than adequate in correctly prediction patient mortality. Overall, all of the models indicated similar variables to be the most important in determining the probability of a patient dying in the hospital. These variables include the Risk of Mortality, the Severity of the Disease, the length of stay at the hospital, and age. Although there were concerns that the data was imbalanced, the results from the balanced random forests show an AUC similar to that of the logistic regression model and the LASSO regression. 

  The advantage of the logistic regression is that the parameter estimates can be used to determine how influential statisitcally significant variables are in determining the probability of death. It is much more detailed, however, the forward selection algorithm in determine which features to include severly limits the scope of variables that was examined. The balanced random forest is not as detailed in its description of the relationships between the parameters and the outcome like logistic regression, but it takes into account for all of the variables present. Because of this, the variable importance plot gives a fuller picture of how all of the variables affect the probability of patient mortality. All of the variables that were considered statistically significant in the logistic regression were considered important in the balanced random forest model, but there were some variables that the forward selection algorithm did not include. The variables that were considered important by the LASSO regression model aligned with what was considered important by the balanced random forest, although cross-validated tunning plot for LASSO regression also shows that the model performs the best with around 59 or 37 non-zero coefficient estimates. T
  
  Overall, all three models show similar results in that the Risk Mortality variable was considered to be the most important in predicting whether the patient would die during hospitalization. Other key variables include the severity of the disease, length of stay at the hospital, and the number of diagnoses on the patient's record. As the AUC for all three models were very high and the misclassification error was low, I am confident that we can use these models to predict patient mortality in hospitals. 



