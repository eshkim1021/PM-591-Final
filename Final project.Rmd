---
title: "Final project"
date: "Due April 30, 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(data.table)
library(mlr)
library(dplyr)
library(randomForest)
library(rpart)

data <- read.csv("NIS2012-200k.csv", header = TRUE, stringsAsFactors = TRUE)

data.dt <- data.frame(data)

```
<br>
The final project requires that you build a predictive model based on real data -- your own or the provided National Impatient data-- and a paper-style short report (2-3 of pages long) describing the problem, the approach(es) taken, and the results. Below is a *guideline* structure for the report. You should use the section breakdown into intro, methods, results, conclusions/discussion but don't have to necessarily include every element listed below within those sections. And you may want to include elements not listed below. Use your judgement.

### Introduction

  The National Impatient Sample (NIS) data, collected by the Healthcare Cost and Utilization Project (HCUP), is the largest publicly available dataset that contains information on inpatient healthcare in hospitals throughout the United States. The NIS is used by policymakers and health officials to make national estimates of healthcare utilization, and observe key features of inpatient care. The NIS was first started in 1998 by the Healthcare Cost and Utilization Project, and contains information such as patient demographics, classification of diseases, total hospital bill, length of stay, and many other features that characterize hospital care. The goal of this assignment will be to build a model to predict impatient mortality an determine what factors contribute a increased risk of death during hospitalization. 

  The data that will be used in this assignment consists of a random subset of 200,000 patients from the 2012 National Impatient Sample. The data was taken from the Healthcare Cost and Ultilization Project (HCUP), which is the largest collection of hospital care data in the United States. The data was taken from discharge records from all hospitals that are participating with the HCUP, and use state guidelines to help identify the hospitals that qualify for the data collection process. 47 states and the District of Columbia participate in the NIS, and data is available for hospitals in those states. The outcome of interest is the inpatient mortality, of whether the patient died during the period of hospitalization. Features such as patient demographic, severity of disease, risk of mortality, and comorbidities were incorpated to determine if a patient was likely to die during hospitalization. This can be used to identify features that increase the risk of patient mortality in hospitals and seek to prevent such deaths in the future. 

1. Describe the problem explaining in particular why prediction is of primary interest (inference could also be of interest but there has to be a good reason for wanting to predict a particular outcome)

2. Describe the data (e.g. data source, data collection, outcome of interest, available features, sample size, missing data, etc.)

### Methods

First the relevant features to the outcome of interest was sorted out from the 175 original features that were present. 

```{r, include = FALSE}
names <- c("DIED", "APRDRG_Risk_Mortality","AGE","APRDRG_Severity","CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","LOS","NCHRONIC","NDX","NEOMAT","PAY1","RACE","YEAR","ZIPINC_QRTL","ORPROC") #list the features that will be included 


refine_data <- data.dt %>% select(names)

```

Then then data was then reevaluated and factors were added when necessary. 

```{r, include = FALSE}
refine_data$DIED <- factor(refine_data$DIED,
                           levels = c(0,1),
                           labels = c("Alive","Died"))

refine_data$APRDRG_Risk_Mortality <- factor(refine_data$APRDRG_Risk_Mortality,
                                            levels = c(0,1,2,3,4), 
                                            labels = c("Not specified","Minor Likelihood","Moderate Likelihood","Major Likelihood","Extreme Likelihood"))


refine_data$APRDRG_Severity <- factor(refine_data$APRDRG_Severity,
                                      levels = c(0,1,2,3,4),
                                      labels = c("Not specified","Minor Loss of Function","Moderate Loss of Function","Major Loss of Function","Extreme Loss of Function"))

factor_names <- c("CM_AIDS","CM_ALCOHOL","CM_ANEMDEF","CM_ARTH","CM_BLDLOSS","CM_CHF","CM_CHRNLUNG","CM_COAG","CM_DEPRESS","CM_DM", "CM_DMCX","CM_DRUG","CM_HTN_C","CM_HYPOTHY","CM_LIVER","CM_LYMPH","CM_LYTES","CM_METS","CM_NEURO","CM_OBESE","CM_PARA","CM_PERIVASC","CM_PSYCH","CM_PULMCIRC","CM_RENLFAIL","CM_TUMOR","CM_ULCER","CM_VALVE","CM_WGHTLOSS","FEMALE","HOSP_DIVISION","NEOMAT")

refine_data[factor_names] <- lapply(refine_data[factor_names],factor)

refine_data$PAY1 <- factor(refine_data$PAY1,
                           levels = c(1,2,3,4,5,6),
                           labels = c("Medicare","Medicaid","Private","Self-Pay","No Charge","Other"))

refine_data$RACE <- factor(refine_data$RACE,
                           levels = c(1,2,3,4,5,6),
                           labels = c("White","Black","Hispanic","Asian","Native American","Other"))


refine_data <- na.omit(refine_data)

refine_data$AGE <- as.integer(refine_data$AGE)

refine_data$LOS <- as.integer(refine_data$LOS)

summary(refine_data)

```

1. Describe any data pre-processing steps (e.g. cleaning, recoding, variable transformation, dealing with missing data, selection of features to be included in your models, etc)

Out of the 175 possible features that were present in the original dataset, only 44 variables were selected to be included in analysis and model building. These 44 include data regarding patient demographics (age, race, gender), comorbidities (such as alcohol abuse and COPD), and the risks of patient mortality. Each variable was examined and was made into factor variables as was appropriate. A majority of the features were converted into dummy variables, however some remained as strings and integers. In examining the missing data, there was less than 1% of the total sample size that was missing from the target variable, whether the patient died. Because the sample was small compared to the dataset, the missing values of the target variable were removed before the analysis.

2. Briefly describe the Machine learning methods you will be using and why they are appropriate for your data (e.g. given the sample size and dimensionality of your training data, are you more concerned about bias or variance?)  You should try and compare at least 3 distinct appropriate methods.


3. Describe how you are splitting the data into testing and training and any resampling strategy used for comparing methods, tuning parameters, and/or model/feature selection.

```{r, include = FALSE}
#make task for log reg 
data_tsk <- makeClassifTask(id = "Paitent Mortality", data = refine_data, target = "DIED")
```



```{r, include = FALSE}
#make log learner 
data_learn_log <- makeLearner("classif.logreg",
                              fix.factors.prediction = TRUE,
                              predict.type = "prob")

holdout_desc <- makeResampleDesc(method = "Holdout", stratify = TRUE)

set.seed(301)
log_split <- makeResampleInstance(holdout_desc,data_tsk, split = 0.7)

log_train <- log_split$train.inds[[1]];log_test <- log_split$test.inds[[1]]


#use forward subset to determine best result 
ctrl_forward <- makeFeatSelControlSequential(method = "sfs", alpha = 0.01)

log_forward_cv <- makeResampleDesc("CV",iters = 5L)

log_forward <- selectFeatures(learner = data_learn_log,
                              task = data_tsk,
                              resampling     = log_forward_cv,
                              measures = auc, 
                              control = ctrl_forward,
                              show.info = TRUE)

#only Risk of Mortality found to be inmportant, but will include other demographic factors 
analyzeFeatSelResult(log_forward)

#Risk of Mortality, Race, and Length of Stay 
forward_log_data <- refine_data %>% select("DIED","APRDRG_Risk_Mortality","LOS","AGE","RACE")

#create new task for the new dataset 
log_for_tsk <- makeClassifTask(id = "Paitent Mortality", data = forward_log_data,
                               target = "DIED")

log_for_train <- train(data_learn_log,log_for_tsk, subset = log_train)

log_for_predict <- predict(log_for_train, task = log_for_tsk, subset = log_test)

calculateROCMeasures(log_for_predict)

performance(log_for_predict, measures = list(mmce,acc))

for_log_crossval <- crossval(data_learn_log,log_for_tsk,iters = 10L, stratify = TRUE, measures = mmce)

for_log_crossval$aggr
```

### __Logistic Regression__

I will be comparing 3 different methods to build a predictive model for patient mortality. The first will be logistic regression model. The logistic regression model is one of the most commonly used and basic binary classifiers. Because the desired goal is to determine if a patient died during their hospitalization, the outcome is a binary outcome.  Given the extremely large sample size of the data with around 200,000 observations, both the training and testing sets will be large enough to ensure an accurate prediction model. 

Forward selection was used to determine the features that will be included in the logistic regression model. According to the forward selection process, only the __APRDRG_Risk_Mortality__, a factor variable that characterizes the risk of patient mortality, was determined to be significant in the data. However, the race variable was also included to determine the effect of patient demographics on mortality. There will only be a couple of features included in the actual logistic prediction model, therefore the model will be a simpler one indicating that the model will have a higher bias. However, the large sample size of the data and the use of cross validation will be used to determine the accuracy of the results. 

```{r}
library('pROC')
data_glm <- glm(DIED~APRDRG_Risk_Mortality + RACE, family = 'binomial',data = forward_log_data[log_train,])

pred_glm <- factor(predict(data_glm,newdata = forward_log_data[log_test, ],type = 'response') >0.5)


predict_prob_train <- predict(data_glm, newdata = forward_log_data[log_train, ])
predict_prob_test <- predict(data_glm,newdata = forward_log_data[log_test, ])

roc_glm_train <- roc(forward_log_data[log_train,]$DIED,predict_prob_train, ci = TRUE, of = 'auc')
roc_glm_test <- roc(forward_log_data[log_test, ]$DIED,predict_prob_test, ci = TRUE, of = 'auc')
```

### __Balanced Random Forests__

The data itself is very unbalanced, with 184,598 patients that were successfully discharged compared to the 3,412 that died in the hospital. This could lead to an optimistically low misclassification error. Therefore, balanced random forests will be used to help rebalance the two binary outcomes. 

```{r}
library(randomForest)


data_rf <- randomForest(DIED~.,data = refine_data[log_train, ],
                        mtry = sqrt(44),
                        ntree = 500,
                        strata = refine_data$DIED,
                        sampsize = c(2274,2274))

data_rf

rf_roc_train <- roc(refine_data[log_train, ]$DIED, data_rf$votes[,1])

auc(rf_roc_train)

rf_predict_test <- predict(data_rf,
                           newdata = refine_data[log_test, ],
                           type = 'prob')

rf_roc_test <- roc(refine_data[log_test, ]$DIED,rf_predict_test[,1])

auc(rf_roc_test)
ci(rf_roc_test)
```

```{r}

plot(rf_roc_train, lwd = 4, col = 'red4',cex.axis = 1.3, cex.lab = 1.3)
plot(rf_roc_test,lwd = 4, col = 'blue4', cex.axis = 1.3, cex.lab = 1.3)

```

```{r}
varImpPlot(data_rf, cex = 0.7, pt.cex = 1.2, n.var = 20, main = "", pch = 16, col = 'red4')

```



```{r}
library('pROC')
data_glm <- glm(DIED~APRDRG_Risk_Mortality + RACE, family = 'binomial',data = forward_log_data[log_train,])

pred_glm <- factor(predict(data_glm,newdata = forward_log_data[log_test, ],type = 'response') >0.5)


predict_prob_train <- predict(data_glm, newdata = forward_log_data[log_train, ])
predict_prob_test <- predict(data_glm,newdata = forward_log_data[log_test, ])

roc_glm_train <- roc(forward_log_data[log_train,]$DIED,predict_prob_train, ci = TRUE, of = 'auc')
roc_glm_test <- roc(forward_log_data[log_test, ]$DIED,predict_prob_test, ci = TRUE, of = 'auc')
```

```{r}
auc(roc_glm_train)
ci(roc_glm_train)

plot(roc_glm_train, lwd = 4, col = 'red4',cex.axis = 1.3, cex.lab = 1.3)
```

```{r}
auc(roc_glm_test)
ci(roc_glm_test)
#have the auc of the test data 
plot(roc_glm_test,lwd = 4, col = 'red4', cex.axis = 1.3, cex.lab = 1.3)
title(main = "ROC Curve for Patient Mortality (Test Data)")

```


```{r}
#random forest 
split_desc <- makeResampleDesc(method = "Holdout",stratify = TRUE)

set.seed(101)
split <- makeResampleInstance(split_desc,data_tsk,split = 0.7)

rf_train <- split$train.inds[[1]];rf_test <- split$test.inds[[1]]

rpart(DIED ~., data = refine_data[rf_train, ], method = "class",control = list(minsplit = 15,minbucket = 5, cp  = 0))
```

4. If applicabble, describe any model/feature selection used. 

4. If applicabble, describe any tuning parameters and how you will be tuning them. 

5. Describe what performance metric(s) you will be using and why.

### Results
1. Present key summaries (table and/or plots, but plots prefered when both available) of your data (e.g. class frequencies if a classification problem)



2. Report training, validation/cross-validation, and test errors. Present cross-validation plots for tuning parameters if available. Report variable importance (e.g. p-values, model coefficients, Random forest and boosting variable importance).

### Conclusions/discussion
Discuss whether and why the prediction model(s) developed achieved sufficient high accuracy to be usefully deployed to predict new observations.

#Additional notes for those using the NIS data
The data provided consists of a random subset of 200,000 patients from 2012 from the National Impatient Sample (NIS) data collected by the Healthcare Cost and Utilization Project (HCUP). You can find information on the HCUP database at <https://www.hcup-us.ahrq.gov>. You can choose to develop a model to predict death during hospitalization also known as impatient mortality (variable DIED in the dataset) or hospital length of stay (variable LOS in the dataset). For extra credit, you can also choose to predict both. The dataset has a relatively large number of variables. In the provided data dictionary I preselected variables (highlighted) which are both availabe (not all variables in the dictionary are available for 2012) which might be relevant for predicting impatient mortality and/or hospital length of stay. Based on their description and additional info from the HCUP site you should choose which variables among the preselected ones you will consider as features/predictors. You don't have to use them all. There maybe variables that are redundant (capture pretty much the same info others already capture), variables that are too complex (e.g. categorical with way too many levels), or that based on your judgment are unlikely to be important. Be aware that the data is real and has not been pre-processed in any way and you will have to do some data cleaning. For example, you should carefully check the variables you consider as possible predictors for correctness of type (e.g. many numeric variables will be read in as factor variables when you use ``read.csv``), outliers, missing observations, nonsensical values, etc.



